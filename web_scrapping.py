# Load dependencies
#import requests
#from bs4 import BeautifulSoup
#import csv
#from datetime import datetime, timedelta
#import time
#import requests
#from bs4 import BeautifulSoup
import pandas as pd

# List to store info from websites
data_for_newsletter = []

# Repeat process for other newsletter
def get_articles_from_fiercehealthcare():
    #url = "https://www.fiercehealthcare.com/health-tech"
    #response = requests.get(url)
    #soup = BeautifulSoup(response.content, "html.parser")

# Adjust according to the HTML structure
    #for article in soup.find_all("article"):


    print("articles scrapped")

#data_for_newsletter.to_csv("data_for_newsletter.csv", index=False)